apiVersion: "serving.kubeflow.org/v1alpha2"
kind: "InferenceService"
metadata:
  controller-tools.k8s.io: "1.0"
  name: microorganism
spec:
  default:
    predictor:
      custom:
        container:
          image: gcr.io/ds-ai-platform/microorganism:v0.1.0-http
          env:
            - name: MODEL_NAME
              value: "microorganism"
            - name: MODEL_BASE_PATH
              value: "/models"
            # - name: STORAGE_URI
            #   value: gs://brain-ds/microorganism_13/model/frozen_inference_graph.pb
          ports:
            - containerPort: 8501
              protocol: TCP
          resources:
            limits:
              cpu: 1
              memory: 4Gi
            requests:
              cpu: 1
              memory: 4Gi

#
# kubectl get kpa -o yaml
# kubectl get ksvc microorganism-predictor-default -o yaml
# kubectl -n inference-test get revision microorganism-predictor-default-ttlpz -o yaml
# kubectl label ns inference-test serving.kubeflow.org/inferenceservice=enabled
# apiVersion: "serving.kubeflow.org/v1alpha2"
# kind: "InferenceService"
# metadata:
#   controller-tools.k8s.io: "1.0"
#   name: microorganism
#   namespace: inference-test
# spec:
#   default:
#     predictor:
#       custom:
#         serviceAccountName: yjkim-kube-admin-sa-gcr
#         container:
#           image: gcr.io/ds-ai-platform/microorganism:v0.1.0-http
#           name: kfserving-container
#           env:
#             - name: MODEL_NAME
#               value: "microorganism"
#             - name: MODEL_BASE_PATH
#               value: "/models"
#             # - name: STORAGE_URI
#             #   value: gs://brain-ds/microorganism_13/model/frozen_inference_graph.pb
#           ports:
#             - containerPort: 8501
#               protocol: TCP
#           resources:
#             limits:
#               cpu: 4
#               memory: 10Gi
#             requests:
#               cpu: 4
#               memory: 10Gi

# # kubectl -n inference-test get revision microorganism-predictor-default-ttlpz -o yaml
# # kubectl label ns inference-test serving.kubeflow.org/inferenceservice=enabled

